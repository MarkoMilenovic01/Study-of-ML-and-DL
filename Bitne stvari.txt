1. Chapter - Machine Learning Basics
What is it? Learning from data Example : Learn to classify the spam mails
Traditional approach would have to write rules, ML approach would automatically learn which words and phrases are good predictors of spam by detecting patterns
Examples : Detecting tumours, Automatically classifying news articles, Automatically flagging offensive comments on discussion forums, Fraud detection etc...

Types of Supervision : 

Supervised (Regression, Classification)

Unsupervised (Clustering, Dimensionality Reduction, Anomaly Detection, Novelty detection, Association rule learning)

Semi-supervised (clustering algorithm may be used to group similar instances together, and then every unlabeled instance can be labeled with the most common label in its cluster, once the whole dataset is labeled, it is possible to use any supervised learning algo)

Self-supervised - Masked images are used as the inputs, the original images are used as the labels... Similar to Audio denoising spectogram

Reinforcement learning - Agent, Environment, Select and Perform actions, and get rewards in return or penalties

Tips
1. Often it's better to firstly do the dimensionality reduction before you feed it to another machine learning algo such as supervised learning algo
2. When doing the novelity detection (is the new instance different from all of the instances in the training set), the training set must be very clean.


Offline Learning vs Online Learning

Learns offline and then when going into production, stops learning and only applies what it has learned
It can be automated fairly easily, if we want to learn from new data we should retrain the model using the new data + the old but is bad on the resources because we are training it from scratch

Learns online as its still learning when going into production, which is fast and cheap, can learn about new data on the fly

Instance based vs Model based

How they generalize?

Instance based : Using a measure of similarity between already known instances and new instances known

Model based : Build a model of these examples and then use that model to make predictions using utility function or cost function

Training a model means running an algorithm to find the model parameters that will make it best fit the training data 


Insufficient Quantity of Training Data, Nonrepresentative Training data (Sampling bias), Data full of errors, outliers and Nonrepresentative, Irrelevant Features (Feature selection - selecting the most important features and Feature extraction - combining existing features to produce a more useful one)

Overfitting the data - if one taxi driver rips me off, i would overgeneralize and conclude that every taxi driver would rip me off - Model performs well on the training data, but it does not generalize well

Overfitting happends when the model is too complex relative to the amount and noisiness of the training data
    - Simplify the model by selecting one with fewer parameters, by reducing the number of attributes in the training data or by constraining the model
    - Gather more training data
    - Reduce the noise in the training data
Constraining the model to make it simpler and reduce the risk of overfitting is called regularization
Goal : Find the right balance between fitting the training data perfectly and keeping the model simple enough to ensure that it will generalize well

Underfitting the data - Occurs when a model is too simple to learn the underlying the structure of the data 
    - Select a more powerful mode, with more parameters
    - Feed better features to the learning algorithm (Feature engineering)
    - Reduce the constraints on the model (by reducing the regularization hyperparameter)

Testing and Validating - classic training and testing split 
If training accuracy is high and test acc is low - that means the model is overfitting
We do not always need 20% split, depends on how many instances we have overall

Hyperparameter tuning and Model selection - Select the model that performs the best (has the best hyperparameters) on the validation set.
Using the cross-validation, using many small validation sets

Tips : Both validation and test set must be as representative as possible of the data you expect in the production.
No free Lunch theorem - The only way to know which model is best is to evaluate them all 

2. Chapter - End to end ML project

1. Look at the big picture
    What are we trying to solve? What task? Use the ML project checklist that is in this repo. What is the business objective
    (Maybe the output of the model will be an input of another ML system etc...) Are there any already made up solutions

    Pipelines : Sequence of data processing components

    Determine what kind of training supervision the model will need : supervised, unsupervised, self suppervised etc...
    Is it classification task or regression task? Should i use batch learning or online learning techniques.

    If there is a lot of data, split batch learning work across multiple servers (using the MapReduce technique) or an online learning technique

    Select a Performance Measure : For regression tasks - RMSE or MAE (mean absolute error used when there are many outlier distritcs)

2. Get the data
In typical situations i get the data from the relationa db or other common data store or a .csv file
And then expect the data with .info() .describe() plot distributions ..
When splitting to a train test split use stratify so that the algorithm isnt biased

3. Explore and visualize the data to gain insights

Look for correlation coefficient between every pair of attributes using the .scatter_matrix() but that only measures linear correlation 
Also, if we combine the attributes to get the attributes with more meaning, there is maybe a possibility that it is good! Feature engineering

4. Prepare the data for machine learning algorithms

Cleaning the data, get rid of corrensponding distritcs, get rid of the whole attribute, set missing values to some value (zero, the mean, the median)

There are also more powerful imputers (the object that handles the missing values) - KNNImputer and IterativeImputer

OneHot encoder for categorical values - if the categorical attribute has a large number of possible cattegories then one-hot encoding will result in a large number of input features which will slow down training

Feature Scaling - Some of the machine learning algo's do not perform well when the input numerical attributes have very different scales
(min-max scaling and standardization) Do this only for the training data with fit and fit_transform and then use the transform for the new data, test set, validation set 
Standardization is much less affected by outliers, also rethink about transforming the value with logarithm if it has a long and heavy tail
Handle the heavy-tailed features - Logarithm or Bucketize

Adding the features with cluster similarity

Use the Pipeline so i can sequentialize the data transformation

5. Select a model and train it
Evaluate it with the Cross-validation From 10 pick 1 for validation and other 9 folds for training 
6. Find-tune your model

Grid-Search and Random-Search for tuning the hyperparameters and tell which parameters you want to tune and use the cross validations
Grid-Searh - Every possible combination from my Search
Random-Search - tries a random subset of combinations
I can also do the feature scores (which features were contributing the most)
7. Present your solution
8. Launch, monitor and maintain your system
Polish the code, write documentation and tests and deploy the model in the production env => Basic way save the best model, transfer the file to production env and load it
If the model is used within the website (REST API)
Deploy on the cloud Google Vertex AI. Save the model to Google Cloud Storage and then head over to Vertex AI and create a new model version pointing to that GCS file and thats it (it will handle anything loadbalancing, scaling etc...)
Write the monitoring code

Working with real data and not artificial datasets OpenML.org, Kaggle.com PapersWithCode.com 

